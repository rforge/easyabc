\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage[colorlinks=true]{hyperref}
\usepackage{a4wide}

\title{\texttt{EasyABC}: a \texttt{R} package to perform efficient approximate Bayesian computation sampling schemes}
\author{Franck Jabot, Thierry Faure, Nicolas Dumoulin}
\date{\pkg{EasyABC} version \Sexpr{packageDescription("EasyABC")[["Version"]]} , \Sexpr{Sys.Date()} }

\SweaveOpts{echo=TRUE,print=TRUE}
% \SweaveOpts{eval=FALSE}

\begin{document}

\maketitle

\tableofcontents
\setcounter{footnote}{1} \footnotetext{This document is included as a
  vignette (a \LaTeX\ document created using the \R function
  \code{Sweave}) of the package \pkg{EasyABC}. It is automatically
  dowloaded together with the package and can be accessed through \R
  typing \code{vignette("EasyABC")}.}  \newpage

\section{Summary}
The aim of this vignette is to present the features of the \texttt{EasyABC} package.
Section \ref{algorithms} describes the different algorithms available in the package.
Section \ref{installation} details how to install the package and the formatting requirements.
Section \ref{example} presents a detailed worked example.

\section{Overview of the package EasyABC}
\label{algorithms}
\texttt{EasyABC} enables to launch various ABC schemes and to retrieve the ouputs of the simulations, so as to perform post-processing treatments with the various R tools available. \texttt{EasyABC} is also able to launch the simulations on multiple cores of a multi-core computer.
Three main types of ABC schemes are available in EasyABC: the standard rejection algorithm of Pritchard et al. (1999), sequential schemes first proposed by Sisson et al. (2007), and coupled to MCMC schemes first proposed by Marjoram et al. (2003).
Four different sequential algorithms are available: the ones of Beaumont et al. (2009), Drovandi and Pettitt (2011), Del Moral et al. (2012) and Lenormand et al. (2012).
Three different MCMC schemes are available: the ones of Marjoram et al. (2003), Wegmann et al. (2009a) and a modification of Marjoram et al. (2003)'s algorithm in which the tolerance and proposal range are determined by the algorithm, following the modifications of Wegmann et al. (2009a).
Details on how to implement these various algorithms with \texttt{EasyABC} are given in the manual pages of each function and an example is detailed in Section \ref{example}. We provide below a short presentation of each implemented algorithm.

\subsection{The standard rejection algorithm of Pritchard et al. (1999)}
This sampling scheme consists in drawing the model parameters in the prior distributions, in using these model parameter values to launch a model simulation and in repeating this two-step procedure \texttt{nb\_simul} times.
At the end of the \texttt{nb\_simul} simulations, the simulations closest to the target (or at a distance smaller than a tolerance threshold) in the space of the summary statistics are retained to form an approximate posterior distribution of the model parameters.
This last step of simulation rejection can be performed with the \texttt{R} package \texttt{abc} (Csill\'ery et al. 2012). A worked example demonstrating how the \texttt{EasyABC} and \texttt{abc} functions can be pipelined is provided in section \ref{example}.

\subsection{Sequential algorithms}
Sequential algorithms for ABC have first been proposed by Sisson et al. (2007). These algorithms aim at reducing the required number of simulations to reach a given quality of the posterior approximation.
The underlying idea of these algorithms is to spend more time in the areas of the parameter space where simulations are frequently close to the target.
Sequential algorithms consist in a first step of standard rejection ABC, followed by a number of steps where the sampling of the parameter space is not anymore performed according to the prior distributions of parameter values.
Various ways to perform this biased sampling have been proposed, and four of them are implemented in the package \texttt{EasyABC}.

\subsection{Coupled to MCMC algorithms}
The idea of ABC-MCMC algorithms proposed by Marjoram et al. (2003) is to perform a Metropolis-Hastings algorithm to explore the parameter space, and in replacing the step of likelihood ratio computation by model simulations.
The original algorithm of Marjoram et al. (2003) is implemented in the method "Marjoram\_original" in \texttt{EasyABC}.
Wegmann et al. (2009) later proposed a number of improvements to the original scheme of Marjoram et al. (2003): they proposed to perform a calibration step so that the algorithm automatically determines the tolerance threshold, the scaling of the summary statistics and the scaling of the jumps in the parameter space during the MCMC.
These improvements have been implemented in the method "Marjoram".
Wegmann et al. (2009) also proposed additional modifications, among which a PLS transformation of the summary statistics. The complete Wegmann et al. (2009)'s algorithm is implemented in the method "Wegmann".

\section{Installation and requirements}
\label{installation}
\subsection{Installing the package}
To install the \texttt{EasyABC} package from \texttt{R}, simply type:
<<eval=FALSE>>= 
install.packages("EasyABC") 
@

Once the package is installed, it needs to be loaded in the current \texttt{R} session to be used:
<<eval=FALSE>>=
library(EasyABC)
@

For online help on the package content, simply type:
<<eval=FALSE>>=
help(package="EasyABC")
@

For online help on a particular command (such as the function \code{ABC_sequential}), simply type:
<<eval=FALSE>>=
help(ABC_sequential)
@


\subsection{The simulation code - for use on a single core}
\label{simulator_single_core}
Users need to develop a simulation code with minimal compatibility constraints. The code can either be a \texttt{R} function or a binary executable file.

If the code is a \texttt{R} function, its argument must be an array of parameter values and it must return an array of summary statistics. If the default option \texttt{use\_seed=TRUE} is chosen, the first parameter value passed to the simulation code corresponds to the seed value to be used by the simulation code to initialize the pseudo-random number generator.  The following parameters are the model parameters.

If the code is a binary executable file, it needs to read the parameter values in a file named 'input' in which each line contains one parameter value, and to output the summary statistics in a file named 'output' in which each summary statistics must be separated by a space or a tabulation.
If the code is a binary executable file, a wrapper \texttt{R} function named 'binary\_model' is available to interface the executable file with the \texttt{R} functions of the \texttt{EasyABC} package (see section \ref{example} below).

Alternatively, users may prefer building a \code{R} function calling their binary executable file. A short tutorial is provided in section \ref{RC_link} to call a \code{C/C++} program.

\subsection{The simulation code - for use with multiple cores}
\label{simulator_several_cores}
Users need to develop a simulation code with minimal compatibility constraints. The code can either be a \texttt{R} function or a binary executable file.

If the code is a \texttt{R} function, its argument must be an array of parameter values and it must return an array of summary statistics. The first parameter value passed to the simulation code corresponds to the seed value to be used by the simulation code to initialize the pseudo-random number generator. The following parameters are the model parameters.

If the code is a binary executable file, it needs to have as its single argument a positive integer \texttt{k}. It has to read the parameter values in a file named 'inputk' (where k is the integer passed as argument to the binary code: 'input1', 'input2'...) in which each line contains one parameter value, and to output the summary statistics in a file named 'outputk' (where k is the integer passed as argument to the binary code: 'output1', 'output2'...) in which each summary statistics must be separated by a space or a tabulation.
This construction avoids multiple cores to read/write in the same files.
If the code is a binary executable file, a wrapper \texttt{R} function named 'binary\_model\_cluster' is available to interface the executable file with the \texttt{R} functions of the \texttt{EasyABC} package (see section \ref{example} below).

Alternatively, users may prefer building a \code{R} function calling their binary executable file. A short tutorial is provided in section \ref{RC_link} to call a \code{C/C++} program.

\subsection{Management of pseudo-random number generators}
To insure that stochastic simulations are independent, the simulation code must either possess an internal way of initializing the seeds of its pseudo-random number generators each time the simulation code is launched.
This can be achieved for instance by initializing the seed to the clock value.
It is often desirable though to have a way to re-run some analyses with similar seed values. 
%#\texttt{EasyABC} offers this possibility by default with the default option \texttt{use\_seed=TRUE,seed\_count=0} where \texttt{seed\_count} can be any integer number.
If this option is chosen, a seed value is provided in the input file as a first (additional) parameter, and incremented by 1 at each call of the simulation code.
This means that the simulation code must be designed so that the first parameter is a seed initializing value.
In the worked example (Section \ref{example}), the simulation code \texttt{trait\_model} makes use of this package default option.

\textit{NB:} Note that when using multicores with the package functions (\texttt{n\_cluster=x} with \texttt{x} larger than 1), the default option \texttt{use\_seed=TRUE} is forced, since the seed value is also used to distribute the tasks to each core.

\subsection{The prior matrix}
A matrix containing the range of the prior distribution of the parameters must be supplied. Each line contains the range values for one parameter.
The first (second) column contains the lower (upper) bound of the range.
Note that fixed variable can be passed to the simulation code by putting the same value in the two columns.
\texttt{EasyABC} only manages uniform prior distribution (it will draw a number between the bounds of the range).
Consequently, to deal with non-uniform prior distribution, users should include parameter transformation in their simulation code.
For instance, in the example below (section \ref{example}), three parameters are exponentially transformed in the simulation code.

\subsection{The target summary statistics}
An array containing the summary statistics of the data must be supplied (for the sequential and MCMC schemes, not for the simple rejection scheme). The statistics must be in the same order as in the simulation outputs.

\subsection{Building a \texttt{R} function calling a \texttt{C/C++} program}
\label{RC_link}
Users having a \texttt{C/C++} simulation code may wish to construct a \texttt{R} function calling their \texttt{C/C++} program, instead of using the provided wrappers (see sections \ref{simulator_single_core} and \ref{simulator_several_cores}).
The procedure is abundantly described in the \href{http://cran.r-project.org/doc/manuals/R-exts.html}{`Writing R Extensions' manual}.
In short, this can be done by:
\begin{itemize}
  \item Adapt your C/C++ program by wrapping your main method into a \texttt{extern "C" \{ â€¦ \}} block. Here is an excerpt of the source code of the trait model provided in this package, in the folder \texttt{src}:
  \begin{verbatim}
  extern "C" {
    void trait_model(double *input,double *stat_to_return){
      // compute output and fill the array stat_to_return
    }
  }
  \end{verbatim} 
  \item Build your code into a binary library (.so under Linux or .dll under Windows) with the \texttt{R CMD SHLIB} command.
  In our example, the command for compiling the trait model and the given output are:
  \begin{verbatim}
  $ R CMD SHLIB trait_model_rc.cpp
  g++ -I/usr/share/R/include -DNDEBUG -fpic -O2 -pipe -g -c trait_model_rc.cpp
  -o trait_model_rc.o
  g++ -shared -o trait_model_rc.so trait_model_rc.o -L/usr/lib/R/lib -lR
  \end{verbatim} 
  \item Load the builded library in your session with the \texttt{dyn.load} function.
  \begin{verbatim}
  > dyn.load("trait_model_rc.so")
  \end{verbatim} 
  \item Use the \texttt{.C} function for calling your program, like we've done in our \texttt{trait\_model} function:
  \begin{verbatim}
  trait_model <- function(input=c(1,500,1,1,1,1)) {
    .C("trait_model",input=input,stat_to_return=array(0,4))$stat_to_return
  }
  \end{verbatim} 
\end{itemize}

\section{A worked example}
\label{example}
\subsection{The trait model}
We consider a simple stochastic ecological model hereafter called \texttt{trait\_model}.
This model represents the stochastic dynamics of an ecological community where each species is represented by a set of traits (i.e. characteristics) which determine its competitive ability.
A detailed description and analysis of the model can be found in Jabot (2010).
The model requires four parameters: an immigration rate $I$, and three additional parameters ($h$, $A$ and $\sigma$) describing the way traits determine species competitive ability.
The model additionnally requires two fixed variables: the total number of individuals in the local community $J$ and the number of traits used $n\_t$.
The model outputs four summary statistics: the species richness of the community $S$, its Shannon's index $H$, the mean of the trait value among individuals $MTV$ and the skewness of the trait value distribution $STV$.

\textit{NB:} Three parameters ($I$, $A$ and $\sigma$) have non-uniform prior distributions: instead, their log-transformed values have a uniform prior distribution.
The simulation code \texttt{trait\_model} therefore takes an exponential transform of the values proposed by \texttt{EasyABC} for these parameters at the beginning of each simulation.

In the following, we will use the values $J=500$ and $n\_t=1$, and uniform prior distributions for $ln(I)$ in $[3;5]$, $h$ in [-25;125], $ln(A)$ in $[ln(0.1);ln(5)]$ and $ln(\sigma)$ in $[ln(0.5);ln(25)]$. The simulation code \texttt{trait\_model} reads sequentially $J$, $I$, $A$, $n\_t$, $h$ and $\sigma$.

\textit{NB:} Note that the fixed variables $J$ and $n\_t$ are included in the prior matrix (lines 1 and 4) with their two columns equal to their fixed values:
  
<<priormatrix>>=
priormatrix=cbind(c(500,3,-2.3,1,-25,-0.7),c(500,5,1.6,1,125,3.2))
@

We will consider an imaginary arbitrary dataset whose summary statistics are $(S,H,MTV,STV) = (100,2.5,20,30000)$:
<<sum_stat_obs>>=
sum_stat_obs=c(100,2.5,20,30000)
@


\subsection{Performing a standard ABC-rejection procedure}
A standard ABC-rejection procedure can be simply performed with the function \texttt{ABC\_rejection}, in precising the number $n$ of simulations to be performed:
<<ABC_rejection>>=
set.seed(1)
n=10
ABC_rej<-ABC_rejection(model=trait_model, prior_matrix=priormatrix, nb_simul=n)
@

Note that a simulation code \texttt{My\_simulation\_code} can be passed to the function \texttt{ABC\_rejection} in several ways depending of its nature:
\begin{itemize}
  \item if it is a \texttt{R} function \\
    \texttt{ABC\_rejection(My\_simulation\_code, prior\_matrix=priormatrix, nb\_simul=n)} 
  \item  if it is a binary executable file and a single core is used (see section \ref{simulator_single_core} for compatibility constraints)\\
    \texttt{ABC\_rejection(binary\_model("./My\_simulation\_code"), prior\_matrix=priormatrix, nb\_simul=n)}
  \item  if it is a binary executable file and multiple cores are used (see section \ref{simulator_several_cores} for compatibility constraints)\\
    \texttt{ABC\_rejection(binary\_model\_cluster("./My\_simulation\_code"), prior\_matrix=priormatrix, nb\_simul=n, n\_cluster=2)}
\end{itemize}

Simulation outputs can be transparently passed to post-processing tools, like the ones proposed by the \texttt{R} package \texttt{abc} (Csill\'ery et al. 2012):
<<abc,eval=FALSE>>=
install.packages("abc")
library(abc)
rej<-abc(sum_stat_obs, ABC_rej$param[,c(2,3,5,6)], ABC_rej$stats, tol=0.3, method="rejection")
# simulations selected:
rej$unadj.values
# their associated summary statistics:
rej$ss
# their normalized euclidean distance to the data summary statistics:
rej$dist
@


\subsection{Performing a sequential ABC scheme}
Other functions of the \texttt{EasyABC} package are used in a very similar manner.
To perform the algorithm of Beaumont et al. (2009), one needs to specify the sequence of tolerance levels $tolerance\_tab$ and the number $nb\_simul$ of simulations to obtain below the tolerance level at each iteration:
<<ABC_Beaumont>>=
n=10
tolerance=c(8,5)
ABC_Beaumont<-ABC_sequential(method="Beaumont", model=trait_model,
prior_matrix=priormatrix, nb_simul=n, summary_stat_target=sum_stat_obs,
tolerance_tab=tolerance)
@

To perform the algorithm of Drovandi and Pettitt (2011), one needs to specify four arguments: the initial number of simulations $nb\_simul$, the final tolerance level $tolerance\_tab$, the proportion $\alpha$ of best-fit simulations to update the tolerance level at each step, and the target proportion $c$ of unmoved particles during the MCMC jump.
Note that default values $alpha=0.5$ and $c=0.01$ are used if not specified, following Drovandi and Pettitt (2011).
<<ABC_Drovandi>>=
n=10
tol=3
alpha=0.5
c=0.7
ABC_Drovandi<-ABC_sequential(method="Drovandi", model=trait_model,
prior_matrix=priormatrix, nb_simul=n, summary_stat_target=sum_stat_obs,
tolerance_tab=tol, alpha=alpha, c=c)
@

To perform the algorithm of Del Moral et al. (2012), one needs to specify five arguments: the initial number of simulations $nb\_simul$, the number $\alpha$ controlling the decrease in effective sample size of the particle set at each step, the number $M$ of simulations performed for each particle, the minimal effective sample size $nb\_threshold$ below which a resampling of particles is performed and the final tolerance level $tolerance\_target$.
Note that default values $alpha=0.5$, $M=1$ and $nb\_threshold=nb\_simul/2$ are used if not specified.
<<ABC_Delmoral>>=
n=10
tol=5
alpha=0.5
ABC_Delmoral<-ABC_sequential(method="Delmoral", model=trait_model, prior_matrix=priormatrix,
nb_simul=n, summary_stat_target=sum_stat_obs, alpha=0.5, tolerance_target=tol)
@

To perform the algorithm of Lenormand et al. (2012), one needs to specify three arguments: the initial number of simulations $nb\_simul$, the proportion $\alpha$ of best-fit simulations to update the tolerance level at each step, and the stopping criterion $p\_acc\_min$.
Note that default values $alpha=0.5$ and $p\_acc\_min=0.05$ are used if not specified, following Lenormand et al. (2012).
<<ABC_Lenormand>>=
n=10
alpha=0.5
paccmin=0.4
n_t=5
ABC_Lenormand<-ABC_sequential(method="Lenormand",model=trait_model,prior_matrix=priormatrix,
nb_simul=n,summary_stat_target=sum_stat_obs,alpha=alpha,p_acc_min=paccmin)
@


\subsection{Performing a ABC-MCMC scheme}
To perform the algorithm of Marjoram et al. (2003), one needs to specify five arguments: the number of sampled points $n\_obs$ in the Markov Chain, the number of chain points between two sampled points $n\_between\_sampling$, the maximal distance accepted between simulations and data $dist\_max$, an array $tab\_normalization$ precising the scale of each summary statistics, and an array $proposal\_range$ precising the maximal distances in each dimension of the parameter space for a jump of the MCMC.
<<ABC_Marjoram_original>>=
n=10
nbetweensampling=1
distmax=8
tabnormalization=c(50,1,20,10000)
proposalrange=c(0,1,0.5,0,50,1)
ABC_Marjoram_original<-ABC_mcmc(method="Marjoram_original",model=trait_model,prior_matrix=priormatrix,n_obs=n,n_between_sampling=nbetweensampling,summary_stat_target=sum_stat_obs,dist_max=distmax,tab_normalization=tabnormalization,proposal_range=proposalrange)
@

To perform the algorithm of Marjoram et al. (2003) in which some of the arguments ($dist\_max$, $tab\_normalization$ and $proposal\_range$) are automatically determined by the algorithm via an initial calibration step, one needs to specify three arguments: the number $n\_calibration$ of simulations to perform at the calibration step, the tolerance quantile $tolerance\_quantile$ to be used for the determination of $dist\_max$ and the scale factor $proposal\_phi$ to determine the proposal range.
These modifications are drawn from the algorithm of Wegmann et al. (2009a), without relying on PLS regressions.
The arguments are set by default to: $n\_calibration=10000$, $tolerance\_quantile=0.01$ and $proposal\_phi=1$.
<<ABC_Marjoram>>=
n=10
nbetweensampling=1
ncalib=10
tolquantile = 0.5
proposalphi=1
ABC_Marjoram<-ABC_mcmc(method="Marjoram",model=trait_model,prior_matrix=priormatrix,
n_obs=n,n_between_sampling=nbetweensampling,summary_stat_target=sum_stat_obs,
n_calibration=ncalib,tolerance_quantile=tolquantile,proposal_phi=proposalphi)
@

To perform the algorithm of Wegmann et al. (2009a), one needs to specify four arguments: the number $n\_calibration$ of simulations to perform at the calibration step, the tolerance quantile $tolerance\_quantile$ to be used for the determination of $dist\_max$, the scale factor $proposal\_phi$ to determine the proposal range and the number of components $numcomp$ to be used in PLS regressions.
The arguments are set by default to: $n\_calibration=10000$, $tolerance\_quantile=0.01$, $proposal\_phi=1$ and $numcomp=0$, this last default value encodes a choice of a number of PLS components equal to the number of summary statistics.
<<#ABC_Wegmann>>=
n=10
nbetweensampling=1
ncalib=10
tolquantile = 0.5
proposalphi=1
ABC_Wegmann<-ABC_mcmc(method="Wegmann",model=trait_model,prior_matrix=priormatrix,n_obs=n,n_between_sampling=nbetweensampling,summary_stat_target=sum_stat_obs,n_calibration=ncalib,tolerance_quantile=tolquantile,proposal_phi=proposalphi,numcomp=0)
@


\subsection{Using multiple cores}
The functions of the package \texttt{EasyABC} can launch the simulations on multiple cores of a computer: users only have to indicate the number of cores they wish to use in the argument \texttt{n\_cluster} of the functions.
The compatibility constraints of the simulation code are slightly different when using multiple cores: please refer to section \ref{simulator_several_cores} for more information.

\section{Troubleshooting and development}
Please send comments, suggestions and bug reports to nicolas.dumoulin@irstea.fr or franck.jabot@irstea.fr 
Any new development of more efficient ABC schemes that could be included in the package is particularly welcome.

\section{Programming Acknowledgements}
The \texttt{EasyABC} package makes use of a number of \texttt{R} tools, among which:

- the \texttt{R} package \texttt{lhs} (Carnell 2012) for latin hypercube sampling.

- the \texttt{R} package \texttt{MASS} (Venables and Ripley 2002) for boxcox transformation.

- the \texttt{R} package \texttt{mnormt} (Genz and Azzalini 2012) for multivariate normal generation.

- the \texttt{R} package \texttt{pls} (Mevik and Wehrens 2011) for partial least square regression.

- the \texttt{R} script for the Wegmann et al. (2009a)'s algorithm drawn from the \texttt{ABCtoolbox} documentation (Wegmann et al. 2009b).


\section{References}
Beaumont, M. A., Cornuet, J., Marin, J., and Robert, C. P. (2009) Adaptive approximate Bayesian computation. \emph{Biometrika},\textbf{96}, 983--990.

Carnell, R. (2012) lhs: Latin Hypercube Samples. R package version 0.10. http://CRAN.R-project.org/package=lhs

Csill\'ery, K., Fran\c cois, O., and Blum, M.G.B. (2012) abc: an r package for approximate bayesian computation (abc). \emph{Methods in Ecology and Evolution}, \textbf{3}, 475--479.

Del Moral, P., Doucet, A., and Jasra, A. (2012) An adaptive sequential Monte Carlo method for approximate Bayesian computation. \emph{Statistics and Computing}, \textbf{22}, 1009--1020.

Drovandi, C. C. and Pettitt, A. N. (2011) Estimation of parameters for macroparasite population evolution using approximate Bayesian computation. \emph{Biometrics}, \textbf{67}, 225--233.

Genz, A., and Azzalini, A. (2012) mnormt: The multivariate normal and t distributions. R package version 1.4-5. http://CRAN.R-project.org/package=mnormt

Jabot, F. (2010) A stochastic dispersal-limited trait-based model of community dynamics. \emph{Journal of Theoretical Biology}, \textbf{262}, 650--661.

Lenormand, M., Jabot, F., Deffuant G. (2012) Adaptive approximate Bayesian computation for complex models. http://arxiv.org/pdf/1111.1308.pdf

Marjoram, P., Molitor, J., Plagnol, V. and Tavar\'e, S. (2003) Markov chain Monte Carlo without likelihoods. \emph{PNAS}, \textbf{100}, 15324--15328.

Mevik, B.-H., and Wehrens, R. (2011) pls: Partial Least Squares and Principal Component regression. R package version 2.3-0. http://CRAN.R-project.org/package=pls

Pritchard, J.K., and M.T. Seielstad and A. Perez-Lezaun and M.W. Feldman (1999) Population growth of human Y chromosomes: a study of Y chromosome microsatellites. \emph{Molecular Biology and Evolution}, \textbf{16}, 1791--1798.

Sisson, S.A., Fan, Y., and Tanaka, M.M. (2007) Sequential Monte Carlo without likelihoods. \emph{PNAS}, \textbf{104}, 1760--1765.

Venables, W.N., and Ripley, B.D. (2002) Modern Applied Statistics with S. Fourth Edition. Springer, New York.

Wegmann, D., Leuenberger, C. and Excoffier, L. (2009a) Efficient approximate Bayesian computation coupled with Markov chain Monte Carlo without likelihood. \emph{Genetics}, \textbf{182}, 1207-1218.

Wegmann, D., Leuenberger, C. and Excoffier, L. (2009b) Using ABCtoolbox. http://cmpg.unibe.ch/software/ abctoolbox/ABCtoolbox\_manual.pdf

\end{document}


